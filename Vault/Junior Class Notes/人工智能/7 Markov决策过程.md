# Markov 决策
Non-Deterministic  Search 非确定性搜索

## 序列式决策问题
- 随机网格世界
- Agent在每个时间步都接受回报（reward）
	每一步都接受小的 “LIVING”回报(  可以是负) § 在终止处获得大的回报(好或坏)
- 目标：最大化回报和

- MDP
对完全可观察的环境，使用马可夫链转移模型和累加回报的这种序列式决策问题，称为马可夫决策过程（MDP）
- 解决序列式决策问题，Agent的效用依赖于一个决策序列
- 在每个状态，Agent都得到一个可正可负但肯定有限的回报，一个环境历史的效用值是对所得到的回报求和。
## Markov决策过程(MDP)和Search问题的差别
- 主要差别：
搜索：后继函数Successor由当前状态产生后继状态
MDP:    由转移函数（概率分布）生成后继状态
- 次要差别：
由最小代价到最大回报
搜索：有一个目标测试；
MDP:  也有终止测试，但是终止可以是一个好的终止，也可能是一个坏的终止。

- 什么是MDP的解---Policies（策略）
- 搜索问题：路径
- MDP问题：策略policy

> “Markov”一般来说意味着给定当前的状态，未来的状态和过去的状态之间是独立的
> 对MDP来说，“Markov”意味着行为的结果仅仅依赖于当前的状态

- Markov  Assumption（马科夫假设）
当前状态仅仅依赖于一个有限的固定数量的以前的状态

- 最优策略
是产生最高期望效用值的策略
需要仔细平衡风险与回报
## 序列状态效用值
- 时间上的效用
有限期的最优策略是非静态的；
无限期的最优策略是静态的；
- 序列的效用
